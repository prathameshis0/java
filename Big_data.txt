sudo apt install openjdk-8-jre

sudo adduser hadoop

sudo usermod -aG sudo hadoop

sudo apt install openssh-server openssh-client -y

sudo su - hadoop

ssh-keygen -t rsa

sudo cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

sudo chmod 640 ~/.ssh/authorized_keys

ssh localhost

wget https://archive.apache.org/dist/hadoop/core/hadoop-3.1.1.tar.gz/

tar -xvzf hadoop-3.1.1.tar.gz

sudo mv hadoop-3.1.1 /usr/local/hadoop

sudo mkdir /usr/local/hadoop/logs

sudo chown -R hadoop:hadoop /usr/local/hadoop

sudo nano ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

source ~/.bashrc

which javac

readlink -f /usr/bin/javac

sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
 			|
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_CLASSPATH+=" $HADOOP_HOME/lib/*.jar"

cd /usr/local/hadoop/lib

sudo wget https://jcenter.bintray.com/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar

cd /home/hadoop

hadoop version

sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
		|
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://0.0.0.0:9000</value>
<description>The default flie system URI</description>
</property>
</configuration>


sudo mkdir -p /home/hadoop/hdfs/{namenode,datanode}

sudo chown -R hadoop:hadoop /home/hadoop/hdfs

sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
			|
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.name.dir</name>
<value>file:///home/hadoop/hdfs/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>file:///home/hadoop/hdfs/datanode</value>
</property>
<property>
<name>dfs.permissions.enabled</name>
<value>false</value>
</property>
</configuration>


sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
			|
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>yarn.app.mapreduce.am.env</name>
<value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
<description>Change this to your hadoop location.</description>
</property>
<property>
<name>mapreduce.map.env</name>
<value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
<description>Change this to your hadoop location.</description>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
<description>Change this to your hadoop location.</description>
</property>
</configuration>


sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
			|
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>


sudo su - hadoop

hdfs namenode -format

start-dfs.sh
start-yarn.sh
jps

http://localhost:9870
http://localhost:8088


-------------------------------------------------------------------
Practical 2
WC_Mapper.java
package com.wordcountproblem;
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
public class WC_Mapper extends MapReduceBase implements
Mapper<LongWritable,Text,Text,IntWritable>{
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
public void map(LongWritable key, Text value,OutputCollector<Text,IntWritable> output,
Reporter reporter) throws IOException{
String line = value.toString();
StringTokenizer tokenizer = new StringTokenizer(line);
while (tokenizer.hasMoreTokens()){
word.set(tokenizer.nextToken());
output.collect(word, one);
}
}

}

WC_Reducer.java
package com.wordcountproblem;
import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

public class WC_Reducer extends MapReduceBase implements
Reducer<Text,IntWritable,Text,IntWritable> {
public void reduce(Text key, Iterator<IntWritable>
values,OutputCollector<Text,IntWritable> output,
Reporter reporter) throws IOException {
int sum=0;
while (values.hasNext()) {
sum+=values.next().get();
}
output.collect(key,new IntWritable(sum));
}
}


WC_Runner.java
package com.wordcountproblem;

import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
public class WC_Runner {
public static void main(String[] args) throws IOException{
JobConf conf = new JobConf(WC_Runner.class);
conf.setJobName("WordCount");
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
conf.setMapperClass(WC_Mapper.class);
conf.setCombinerClass(WC_Reducer.class);
conf.setReducerClass(WC_Reducer.class);
conf.setInputFormat(TextInputFormat.class);
conf.setOutputFormat(TextOutputFormat.class);
FileInputFormat.setInputPaths(conf,new Path(args[0]));
FileOutputFormat.setOutputPath(conf,new Path(args[1]));
JobClient.runJob(conf);
}
}



Compiling
javac -classpath hadoop-core-1.2.1.jar -d wordcountproblem WC_Mapper.java WC_Reducer.java WC_Runner.java
$ jar -cvf wordcountproblem.jar -C wordcountproblem /

text file--> HDFS is a stroage unit of hadoop MapReduce is a processing tool of hadoop

Use cd command to go to directory were wordcountproblem.jar is saved

hadoop jar wordcountproblem .jar com.wordcountproblem.WC_Runner /test/data.txt /r_output
ls

hdfs dfs -cat /r_output/part-00000
--------------------------------------------------------------------------------
The trigram model is a type of n-gram language model used in natural language processing (NLP). In a trigram model, the probability of a word occurring is based on the two preceding words.

The trigram model assumes that the probability of a word depends only on the previous two words, not on the entire history.

from collections import defaultdict
import random
def generate_trigrams(text):
word=text.split()
trigram_model=defaultdict(list)
for i in range(len(words)-2):
key=(words[i],words[i+1])
next_word=words[i+2]
trigram_model[key].append(next_word)
return trigram_model

def generate_text(model,start_words,num_words=10):
if start_words not in model:
return "The given start word are not in the model."

current_words=list(start_words)
output=list(current_words)

for _ in range(num_words):
next_words=model.get(truple(current_words))
if not next_words:
break
next_word=random.choice(next_words)
output.append(next_word)
current_words=[current_words[1],next_word]
return ' '.join(output)

input_text="I wish I may I wish I might"

model= generate_trigrams(input_text)


------------------------------------------------------------------------
Practical No 4

wget https://downloads.apache.org/spark/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz

tar-xzf spark-4.0.0-bin-hadoop3.tgz

mv spark-4.0.0-bin-hadoop3 ~/spark

nano ~/.bashrc

export SPARK_HOME=~/spark
export PATH=$SPARK_HOME/bin:$PATH

source ~/.bashrc

spark-shell

import org.apache.spark.graphx._
case class User(name: String, age:Int)
val users=List((1L,User("Alex",26)),(2L,User("Bill",42)),(3L,User("Carol",18)),(4L,User("Dave",16)),(5L,User("Eve",45)),(6L,User("Farell",30)),(7L,User("Garry",32)),(8L,User("Harry",36)),(9L,User("Ivan",28)),(10L,User("Jill",48)))

val userRDD=sc.parallelize(users)
val follows=List(Edge(1L,2L,1),Edge(2L,3L,1),Edge(3L,1L,1),Edge(3L,4L,1),Edge(3L,5L,1),Edge(4L,5L,1),Edge(6L,5L,1),Edge(7L,6L,1),Edge(6L,8L,1),Edge(7L,8L,1),Edge(7L,9L,1),Edge(9L,8L,1),Edge(8L,10L,1),Edge(10L,9L,1),Edge(1L,1L,1))

val followsRDD=sc.parallelize(follows)

val defaultUser = User("Icarus",
22)
val socialgraph = Graph (usersRDD,
followsRDD, defaultUser)

socialgraph.numEdges
socialgraph.numVertices
socialgraph.inDegrees.collect
socialgraph.outDegrees.collect


--------------------------------------------------------------------------
Practical 5
# switch to Hadoop user

# Install scala-sbt via the latest command available in the terminal

echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list

echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee /etc/apt/sources.list.d/sbt_old.list

curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add

sudo apt-get update

sudo apt-get install sbt

# enter sbt in the terminal to verify installation

# create the scala program for exception handling
$ nano ExceptionHandlingTest.scala

import org.apache.spark.sql.SparkSession
object ExceptionHandlingTest {
def main(args: Array[String]): Unit = {
val spark = SparkSession
.builder
.appName("ExceptionHandlingTest")
.master("local[*]")
.getOrCreate()

val rdd=spark.sparkContext.parallelize(0 until spark.sparkContex.defaultParallelism)
rdd.foreach { i =>
try{
if(math.random>0.75){
throw new Exception("Testing exception handling")
}else{
println(s"Task $i completed successfully")
}
}catch{
case e:Exception=>
	println(s"Exception in task $i:${e.getMessage}")
}
}
spark.stop()
}
}

$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-worker.sh spark://localhost7077

mkdir ~/ExceptionHandlingProject
cd ~/ExceptionHandlingProject
mkdir -p src/main/scala
 
mv ~/ExceptionHandlingTest.scala src/main/scala/
nano build.sbt

scalaVersion="2.12.18"

libraryDependencies ++= Seq(
"org.apache.spark"%%"spark-core"%"3.4.1",
"org.apache.spark"%%"spark-sql"%"3.4.1"
)

sbt package

spark-submit \
	--class ExceptionHandlingTest \
	--master local[*] \
	target/scala-2.12/exceptionhandlingproject_2.12.0.1.0-SNAPSHOT.jar


------------------------------------------------------------------------------
Practical 6
mkdir ~/networkwordcount
cd ~/networkwordcount
nano NetworkWordCount.scala

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
object NetworkWordCount {
def main(args: Array[String]): Unit = {
val sparkConf = new SparkConf().setMaster("local[2]")setAppName("NetworkWordCount")
val ssc = new StreamingContext(sparkConf, Seconds(10))
val lines = ssc.socketTextStream("localhost",9999)
val words = lines.flatMap(_.split(" "))
val tuples = words.map(word => (word ,1))
val wordCounts = tuples.reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
}
}

nano build.sbt

name:="networkwordcount"
version:="1.0.0"
scalaVersion:="2.12.18"

libraryDependencoes ++= Seq(
"org.apache.spark""%%"spark-core"%"3.4.1"
"org.apache.spark""%%"spark-sql"%"3.4.1"
"org.apache.spark""%%"spark-streaming"%"3.4.1"
)

sbt package

$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-workers.sh spark://mohit-virtual-machine:7077

spark-submit \
	--class NetworkWordCount \
	--master spark://mohit-virtual-machine:7077 \
	target/scala-2.12/networkwordcount_2.12-1.0.0.jar

open separate terminal to start netscape server
nc -lk 9999
--------------------------------------------------------------------------------
Practical 8
pig

wget https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz

tar -xvzf pig-0.17.0.tzr.gz
sudo mv /home/hadoop/pig ~/
nano ~/.bashrc

source ~/.bashrc

export PIG_HOME=/home/big/pig
export PATH=$PATH:$PATH_HOME/bin
export PIG_CLASSPATH=$HADOOP_HOME/etc/hadoop

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH

pig -version

create a txt file
nano /home/mohit/textfile.txt
hello world
hello hadoop
pig is powerful

pig -x local

run this script for wordcount problem 
lines = LOAD '/home/mohit/textfile.txt' AS (line:chararray);
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;
grouped = GROUP words BY word;
wordcount = FOREACH grouped GENERATE group, COUNT(words);
DUMP wordcount;



----------------------------------------------------------------
Practical 7
start Hadoop
start-dfs.sh
start-yarn.sh

wget https://archive.apache.org/dist/hbase/2.4.1/hbase-2.4.1-bin.tar.gz

tar -xvf hbase-2.4.1-bin.tar.gz

mv hbase-2.4.1 hbase
nano ~/.bashrc
export HBASE_HOME=/home/hadoop/hbase
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HBASE_HOME/bin

source ~/.bashrc

nano ~/hbase/conf/hbase-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

Open HBase-site.xml and mention the below properties in the file.
nano ~/hbase/conf/hbase-site.xml

<configuration>
 <property>
 <name>hbase.rootdir</name>
 <value>hdfs://localhost:9000/hbase</value>
 </property>
 <property>
 <name>hbase.cluster.distributed</name>
 <value>true</value>
 </property>
 <property>
 <name>hbase.zookeeper.quorum</name>
 <value>localhost</value>
</property>
 <property>
 <name>dfs.replication</name>
 <value>1</value>
 </property>
 <property>
 <name>hbase.zookeeper.property.clientPort</name>
 <value>2181</value>
 </property>
 <property>
 <name>hbase.zookeeper.property.dataDir</name>
 <value>/home/hadoop/hbase/zookeeper</value>
 </property>
</configuration>

start-hbase.sh

hbase shell

create 'emp', 'pri_data', 'pro_data'

put 'emp', '1', 'pri_data:name', 'Andy'
put 'emp', '1', 'pri_data:age', '22'
put 'emp', '1', 'pro_data:post', 'asst. manager'
put 'emp', '1', 'pro_data:salary', '40k'
put 'emp', '2', 'pri_data:name', 'Icarus'
put 'emp', '2', 'pri_data:age', '22'
put 'emp', '2', 'pro_data:post', 'manager'

Retrieve Data (GET)
get 'emp', '1'
get 'emp', '2'


delete 'emp', '1', 'pri_data:city'
deleteall 'emp', '1'

scan 'emp'

list