-->Practical-1: Write a program to implement sentence segmentation and word tokenization


#pip intall nltk
#py -m pip install --upgrade pip 

#nltk.download('punkt') 
#nltk.download('wordnet')

from nltk.tokenize import word_tokenize 
text="God is Great! I won a lottery." print("The words are",word_tokenize(text))

from nltk.tokenize import sent_tokenize text="God is Great! I won a lottery." print("The sentences are",sent_tokenize(text))


-->Practical-2: Write a program to Implement stemming and lemmatization

import nltk
from nltk.stem import WordNetLemmatizer 
wordnet_lemmatizer = WordNetLemmatizer() 
text=input("Enter words for Lemmatizing") 
tokenization= nltk.word_tokenize(text)
# v verb, a adjective, n noun in lemmatize parameter
for w in tokenization:
print("Lemma for {} is {}".format(w,wordnet_lemmatizer.lemmatize(w,'v')))

-->Practical-3: Write a program to Implement a tri-gram model

import nltk 
nltk.download('punkt')
from nltk.tokenize import word_tokenize 
from nltk import FreqDist
import pandas as pd

sample = "Hello i am a Person who love about netwoking and Coding and Computer Fundamentals."
sample_tokens = nltk.word_tokenize(sample) 
print('\n Sample Tokens:',sample_tokens)
print('\n Type of Sample Tokens:',type(sample_tokens))
print('\n Length of Sample Tokens:',len(sample_tokens))
sample_freq =FreqDist(sample_tokens) 
tokens=[]
sf=[]
for i in sample_freq:
  tokens.append(i) 
  sf.append(sample_freq[i])
df = pd.DataFrame({'Tokens':tokens,'Frequency':sf}) 
print('\n',df)
print('\n Bigrams:',list(nltk.bigrams(sample_tokens))) 
print('\n Trigrams:',list(nltk.trigrams(sample_tokens))) 
print('\n Ngrams(4):',list(nltk.ngrams(sample_tokens,4)))

-->Practical-4: Write a program to Implement PoS tagging using HMM & Neural Model

import nltk
from collections import Counter
text="Guru(9 is one of the best sites to learn WEB,SAP,Ethical Hacking and much more online." 
lower_case=text.lower()
tokens=nltk.word_tokenize(lower_case) 
tags=nltk.pos_tag(tokens) 
print(tags) 
counts=Counter(tag for word,tag in tags)
for tag in tags:
  print(tag)
print(counts)
fd=nltk.FreqDist(tokens) 
fd.plot()
fd1=nltk.FreqDist(counts) 
fd1.plot()

-->Practical-5: Write a program to Implementsyntactic parsing of a given text

import nltk
nltk.download('average_preceptron_tagger')

from nltk import pos_tag,word_tokenize,RegexpParser

sentence='Reliance Retail acquires majority stake in designer brand Abraham & Thomson'
tokens=word_tokenize(sentence)
tags=pos_tag(tokens)
grammer="NP:{<NN>?<DT>*<NN>}"
chunker=RegexpParser(grammer)
result=chunker.parse(tags)
print(result)
result.draw()

-->Practical 6:Dependency parsing 
import nltk
import spacy
from spacy import displacy
nlp=spacy.load('en_core_web_sm')
sentence="The Browm fox is quick and he is jumping over the lazy dog"
doc=nlp(sentence)
print("{:15}|{:8}|{:<15}|{:<20}".format('Token','Relation','Head','Children'))
print("-"*70)
for token in doc:
  print("{:15}|{:8}|{:<15}|{:<20}".format(str(token.text),str(token.dep_),str(token.head.text),str([child for child in token.children])))

displacy.render(doc,style='dep',jupyter=True)


--> Practical-7:(NER)
import spacy
from spacy import displacy
import pandas as pd
NER=spacy.load('en_core_web_sm')
text="Apple acquired zoom-in-china-on-wednesday-6th May 2020.\ This news made Apple and Google stock jump by 5% on Dow Jones Index in -the \ United State of America."
doc=NER(text)
entities=[]
labels=[]
position_start=[]
position_end=[]
for ent in doc.ents:
  entities.append(ent)
  labels.append(ent.label_)
  position_start.append(ent.start_char)
  position_end.append(ent.end_char)
df=pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start,'Position_End':position_end})
print(df)
displacy.render(doc,style='dep',jupyter=True)

-->Practical-8: Write a program to Implement Text Summarization for the given sample text


#p8 summary
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')

text="NLP text summarization is the process of breaking down lengthy text into digestible paragraphs or ssentence. This method extract vital information while also preserving the meaning of the text. This reduces the time required for grasping lengthy pieces such as articles without losing vital information. Text summarization is process of creating a concise,coherent, and fluent summary of longer text document, which involves underlining the document's key points."
word=word_tokenize(text)
sents=sent_tokenize(text)
stopwords=set(stopwords.words('english'))
freqTable=dict()
for word in word:
  word=word.lower()
  if word in stopwords:
    continue
  if word in freqTable:
    freqTable[word]+=1
  else:
    freqTable[word]=1
sent_Value=dict()
for sent in sents:
  for word,freq in freqTable.items():
    if word in sent.lower():
      if sent in sent_Value:
        sent_Value[sent]+=freq
      else:
        sent_Value[sent]=freq
sum_Values=0
for sent in sent_Value:
  sum_Values+=sent_Value[sent]
  average=int(sum_Values/len(sent_Value))

summary=""
for sent in sents:
  if sent in sent_Value and sent_Value[sent]>(1.2*average):
    summary+=" "+sent
print(summary)